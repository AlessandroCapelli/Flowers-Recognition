{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Source Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0OfFtAWDfXm"
      },
      "source": [
        "# Project of Advanced Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxOgXMOVDfXt"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9TP5jjhZjDm"
      },
      "source": [
        "!pip install -q tensorflow-model-optimization\n",
        "!pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcwZHDfjDfXv"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "import tarfile\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential, clone_model, load_model\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "tensorflow.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE5WblbCDfXw"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l9cxCJuDfXx"
      },
      "source": [
        "def get_image_names(tgz):\n",
        "    with tarfile.open(tgz) as file:\n",
        "        return [i.name for i in file.getmembers() if i.isfile()]\n",
        "\n",
        "def show_random_images(n=4, size=10, label=None):\n",
        "    if(label != None):\n",
        "        images = df[df['label'] == str(label)].sample(n=(n*n))\n",
        "    else:\n",
        "        images = df.sample(n=(n*n))\n",
        "        \n",
        "    plt.figure(figsize=(size, size))\n",
        "    for index, path in enumerate(images['id'].values):\n",
        "        plt.subplot(n, n, index+1)\n",
        "        plt.imshow(PIL.Image.open(DATA_PATH + 'images/' + path))\n",
        "        plt.title('Label: ' + str(images['label'].values[index]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def show_random_images_aug(aug, n=2, size=10):\n",
        "    x, y = aug.next()\n",
        "    \n",
        "    plt.figure(figsize=(size, size))\n",
        "    for i in range(0, (n*n)):\n",
        "        plt.subplot(n, n, i+1)\n",
        "        plt.imshow(x[i])\n",
        "        plt.title('Label: ' + str(np.where(y[i] == 1)[0][0]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['accuracy'], 'orange', label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], 'royalblue', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['precision'], 'orange', label='Training Precision')\n",
        "    plt.plot(history.history['val_precision'], 'royalblue', label='Validation Precision')\n",
        "    plt.title('Training and Validation Precision')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['recall'], 'orange', label='Training Recall')\n",
        "    plt.plot(history.history['val_recall'], 'royalblue', label='Validation Recall')\n",
        "    plt.title('Training and Validation Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['f1_score'], 'orange', label='Training F1-Score')\n",
        "    plt.plot(history.history['val_f1_score'], 'royalblue', label='Validation F1-Score')\n",
        "    plt.title('Training and Validation F1-Score')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1-Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['loss'], 'orange', label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], 'royalblue', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def true_positives(y_true, y_pred):\n",
        "    return tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    predicted_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_pred, 0, 1)))\n",
        "\n",
        "    return true_positives(y_true, y_pred) / (predicted_positives + tensorflow.keras.backend.epsilon())\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    possible_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_true, 0, 1)))\n",
        "\n",
        "    return true_positives(y_true, y_pred) / (possible_positives + tensorflow.keras.backend.epsilon())\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    possible_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_true, 0, 1)))\n",
        "    predicted_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_pred, 0, 1)))\n",
        "\n",
        "    return 2*(precision(y_true, y_pred)*recall(y_true, y_pred))/(precision(y_true, y_pred)+recall(y_true, y_pred)+tensorflow.keras.backend.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgdgpW-oDfXy"
      },
      "source": [
        "## Configuration parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WptElZuQEbWs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXhIxp-tDfX0"
      },
      "source": [
        "DATA_PATH = '/content/drive/My Drive/data/' # local run only: './data/'\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "TEST_RATIO = 0.3\n",
        "VALIDATION_RATIO = 0.2\n",
        "NUM_CLASSES = 102\n",
        "IMG_SIZE = 250\n",
        "IMG_CHANNELS = 3\n",
        "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "VERBOSE = 1\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 5\n",
        "\n",
        "CHECKPOINT = ModelCheckpoint(\n",
        "    (DATA_PATH + 'model.hdf5'), \n",
        "    monitor=['val_accuracy'],\n",
        "    verbose=VERBOSE, \n",
        "    mode='max'\n",
        ")\n",
        "EARLYSTOP = EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    patience=PATIENCE, \n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D_pQ291DfX0"
      },
      "source": [
        "## Base models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XPBeUxQDfX0"
      },
      "source": [
        "model_vgg = VGG16(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=INPUT_SHAPE\n",
        ")\n",
        "model_vgg.trainable = False\n",
        "\n",
        "model_vgg.summary()\n",
        "\n",
        "model_xception = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=INPUT_SHAPE\n",
        ")\n",
        "model_xception.trainable = False\n",
        "\n",
        "model_xception.summary()\n",
        "\n",
        "model_resnet = ResNet50V2(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=INPUT_SHAPE\n",
        ")\n",
        "model_resnet.trainable = False\n",
        "\n",
        "model_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of9vKuJYDfX3"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3zJszVJDfX3"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['id'] = sorted(get_image_names(DATA_PATH + 'images.tgz'))\n",
        "df['label'] = scipy.io.loadmat(DATA_PATH + 'labels.mat')['labels'][0] - 1\n",
        "df['label'] = df['label'].astype('str')\n",
        "\n",
        "# tarfile.open(DATA_PATH + 'images.tgz').extractall(DATA_PATH + 'images/') # only first run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M9eZEidDfX4"
      },
      "source": [
        "print('Size of dataset:\\n', df.count())\n",
        "\n",
        "print('\\nList of images:\\n', os.listdir(DATA_PATH + 'images/jpg')[:10])\n",
        "\n",
        "print('\\nDataframe:\\n', df.head(10))\n",
        "\n",
        "print('\\nNumber of classes:\\n', df['label'].nunique())\n",
        "\n",
        "print('\\nImages for each class:\\n', df['label'].value_counts())\n",
        "\n",
        "print('\\nSample images:\\n')\n",
        "show_random_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8_KHGSbDfX6"
      },
      "source": [
        "## Dataset split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7n9X16vDfX6"
      },
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(\n",
        "    df['id'], \n",
        "    df['label'], \n",
        "    test_size=TEST_RATIO, \n",
        "    random_state=SEED, \n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "train_X, validation_X, train_y, validation_y = train_test_split(\n",
        "    train_X, \n",
        "    train_y, \n",
        "    test_size=(VALIDATION_RATIO/(1-TEST_RATIO)), \n",
        "    random_state=SEED,\n",
        "    stratify=train_y\n",
        ")\n",
        "\n",
        "train = pd.DataFrame(train_X)\n",
        "train['label'] = train_y\n",
        "\n",
        "validation = pd.DataFrame(validation_X)\n",
        "validation['label'] = validation_y\n",
        "\n",
        "test = pd.DataFrame(test_X)\n",
        "test['label'] = test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOaUF4oVDfX7"
      },
      "source": [
        "print('Size of train set:\\n', train.shape)\n",
        "\n",
        "print('\\nDataframe of the train set:\\n', train.head(10))\n",
        "\n",
        "print('\\nImages for each class of the train set:\\n', train['label'].value_counts())\n",
        "\n",
        "print('\\nSize of validation set:\\n', validation.shape)\n",
        "\n",
        "print('\\nDataframe of the validation set:\\n', validation.head(10))\n",
        "\n",
        "print('\\nImages for each class of the validation set:\\n', validation['label'].value_counts())\n",
        "\n",
        "print('\\nSize of test set:\\n', test.shape)\n",
        "\n",
        "print('\\nDataframe of the test set:\\n', test.head(10))\n",
        "\n",
        "print('\\nImages for each class of the test set:\\n', test['label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7jzcvZ3DfX7"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LULYp5bNDfX7",
        "scrolled": false
      },
      "source": [
        "pd.DataFrame(df['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eh__ouoDfX8"
      },
      "source": [
        "pd.DataFrame(train['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYuHxLzaDfX9"
      },
      "source": [
        "pd.DataFrame(validation['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eule5C0yDfX9"
      },
      "source": [
        "pd.DataFrame(test['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9niX7G_DfX-"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9g5XrBQDfX-"
      },
      "source": [
        "train_aug_datagen = ImageDataGenerator(\n",
        "    rotation_range=45, \n",
        "    width_shift_range=0.1, \n",
        "    height_shift_range=0.1, \n",
        "    brightness_range=[0.5, 1.5], \n",
        "    shear_range=0.15, \n",
        "    zoom_range=[0.75, 1.25], \n",
        "    fill_mode=\"nearest\", \n",
        "    horizontal_flip=True, \n",
        "    rescale=1./255\n",
        ")\n",
        "train_aug = train_aug_datagen.flow_from_dataframe(\n",
        "    dataframe=train, \n",
        "    directory=(DATA_PATH + 'images/'), \n",
        "    x_col='id', \n",
        "    y_col='label', \n",
        "    target_size=(IMG_SIZE, IMG_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "validation_aug_datagen = ImageDataGenerator(\n",
        "    fill_mode=\"nearest\", \n",
        "    rescale=1./255\n",
        ")\n",
        "validation_aug = validation_aug_datagen.flow_from_dataframe(\n",
        "    dataframe=validation, \n",
        "    directory=(DATA_PATH + 'images/'), \n",
        "    x_col='id', \n",
        "    y_col='label', \n",
        "    target_size=(IMG_SIZE, IMG_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "test_aug_datagen = ImageDataGenerator(\n",
        "    fill_mode=\"nearest\", \n",
        "    rescale=1./255\n",
        ")\n",
        "test_aug = test_aug_datagen.flow_from_dataframe(\n",
        "    dataframe=test, \n",
        "    directory=(DATA_PATH + 'images/'), \n",
        "    x_col='id', \n",
        "    y_col='label', \n",
        "    target_size=(IMG_SIZE, IMG_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJypoYbADfX-",
        "scrolled": false
      },
      "source": [
        "print('Sample augmented images of the train set:')\n",
        "show_random_images_aug(aug=train_aug)\n",
        "\n",
        "print('Sample augmented images of the validation set:')\n",
        "show_random_images_aug(aug=validation_aug)\n",
        "\n",
        "print('Sample augmented images of the test set:')\n",
        "show_random_images_aug(aug=test_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjcuvaqaDfX_"
      },
      "source": [
        "## Model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogdjWGUEDfX_"
      },
      "source": [
        "tensorflow.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D5NCVS-8J45"
      },
      "source": [
        "### Model creation for manual tuning of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkDW8jfdDfYA",
        "scrolled": true
      },
      "source": [
        "def create_model(BASE_MODEL, LR, DROPOUT_RATE, NUM_UNITS, ACTIVATION, REGULARIZER=None):\n",
        "    BASE_MODEL_TEMP = clone_model(BASE_MODEL)\n",
        "    BASE_MODEL_TEMP.set_weights(BASE_MODEL.get_weights())\n",
        "    BASE_MODEL_TEMP.trainable = True\n",
        "\n",
        "    # fine_tune_at = int(round(len(BASE_MODEL_TEMP.layers)))\n",
        "    # for layer in BASE_MODEL_TEMP.layers[:fine_tune_at]:\n",
        "    #     layer.trainable = False\n",
        "    \n",
        "    # for layer in BASE_MODEL_TEMP.layers: \n",
        "    #     layer.build(layer.input_shape)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(BASE_MODEL_TEMP)\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(\n",
        "        NUM_UNITS, \n",
        "        activation=ACTIVATION, \n",
        "        kernel_regularizer=REGULARIZER)\n",
        "    )\n",
        "    model.add(Dropout(DROPOUT_RATE))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', \n",
        "        optimizer=Adam(lr=LR), \n",
        "        metrics=[precision, recall, f1_score, AUC(), 'accuracy']\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_LYg48w8e5x"
      },
      "source": [
        "### Model creation for automatic tuning of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkjVT1t38iBS"
      },
      "source": [
        "def build_model(hp):\n",
        "    BASE_MODEL_TEMP = clone_model(model_resnet)\n",
        "    BASE_MODEL_TEMP.set_weights(model_resnet.get_weights())\n",
        "    BASE_MODEL_TEMP.trainable = True\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(BASE_MODEL_TEMP)\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(\n",
        "        hp.Int('NUM_UNITS', 768, 1024, step=128, default=768),\n",
        "        activation=hp.Choice(f'ACTIVATION', ['elu', 'relu']), \n",
        "        kernel_regularizer=l2(3e-1))\n",
        "    )\n",
        "    model.add(Dropout(hp.Float(f'DROPOUT_RATE', 0.4, 0.7, step=0.1, default=0.5)))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', \n",
        "        optimizer=Adam(\n",
        "            lr=hp.Choice(f'LR', [1e-5, 2e-5]), \n",
        "            beta_1=hp.Float(f'BETA_1', 0.85, 0.95, step=0.05, default=0.9), \n",
        "            beta_2=hp.Float(f'BETA_2', 0.9987, 0.9999, step=0.0004, default=0.9991)\n",
        "        ), \n",
        "        metrics=[precision, recall, f1_score, AUC(), 'accuracy']\n",
        "    )\n",
        "  \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNp7EooBDfYA"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPYcaC0x7JSH"
      },
      "source": [
        "### Naive approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejsEJO1wDfYA"
      },
      "source": [
        "PARAMS = {\n",
        "    'BASE_MODEL': [{'MODEL': model_resnet, 'NAME': 'RESNET50V2'}], # [{'MODEL': model_vgg, 'NAME': 'VGG16'}, {'MODEL':  model_xception, 'NAME': 'XCEPTION'}, {'MODEL': model_resnet, 'NAME': 'RESNET50V2'}],\n",
        "    'LR': [2e-5], # [1e-3, 1e-4],\n",
        "    'EPOCHS': [50],\n",
        "    'DROPOUT_RATE': [0.45], # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "    'NUM_UNITS': [256], # [128, 256, 512],\n",
        "    'ACTIVATION': ['elu'], # ['elu', 'relu'],\n",
        "    'REGULARIZER': [{'REGULARIZER': l2(3e-1), 'NAME': 'L2(0.3)'}] #Â [{'REGULARIZER': l1(5e-3), 'NAME': 'L1(0.005)'}, {'REGULARIZER': l2(1e-2), 'NAME': 'L2(0.01)'}, {'REGULARIZER': l1(5e-4), 'NAME': 'L1(0.0005)'}, {'REGULARIZER': l2(1e-3), 'NAME': 'L2(0.001)'}]\n",
        "}\n",
        "\n",
        "with open(DATA_PATH + 'results.csv', mode='w') as results:\n",
        "    results_writer = csv.writer(\n",
        "        results, delimiter=',',\n",
        "        quotechar='\"', \n",
        "        quoting=csv.QUOTE_MINIMAL\n",
        "    )\n",
        "    results_writer.writerow([\n",
        "        'EPOCHS', \n",
        "        'BATCH SIZE', \n",
        "        'BASE MODEL', \n",
        "        'DROPOUT RATE', \n",
        "        'LR', \n",
        "        'NUM UNITS', \n",
        "        'ACTIVATION', \n",
        "        'REGULARIZER', \n",
        "        'LOSS', \n",
        "        'ACCURACY', \n",
        "        'PRECISION', \n",
        "        'RECALL', \n",
        "        'F1-SCORE', \n",
        "        'VALIDATION LOSS', \n",
        "        'VALIDATION ACCURACY', \n",
        "        'VALIDATION PRECISION', \n",
        "        'VALIDATION RECALL', \n",
        "        'VALIDATION F1-SCORE'\n",
        "    ])\n",
        "\n",
        "for EPOCHS in PARAMS['EPOCHS']:\n",
        "    for BASE_MODEL in PARAMS['BASE_MODEL']:\n",
        "        for DROPOUT_RATE in PARAMS['DROPOUT_RATE']:\n",
        "            for REGULARIZER in PARAMS['REGULARIZER']:\n",
        "                for LR in PARAMS['LR']:\n",
        "                    for NUM_UNITS in PARAMS['NUM_UNITS']:\n",
        "                        for ACTIVATION in PARAMS['ACTIVATION']:\n",
        "                            with open(DATA_PATH + 'results.csv', mode='a') as results:\n",
        "                                results_writer = csv.writer(\n",
        "                                    results, \n",
        "                                    delimiter=',', \n",
        "                                    quotechar='\"', \n",
        "                                    quoting=csv.QUOTE_MINIMAL\n",
        "                                )\n",
        "\n",
        "                                model = create_model(\n",
        "                                    BASE_MODEL['MODEL'], \n",
        "                                    LR, \n",
        "                                    DROPOUT_RATE, \n",
        "                                    NUM_UNITS, \n",
        "                                    ACTIVATION, \n",
        "                                    REGULARIZER['REGULARIZER']\n",
        "                                )\n",
        "                                model.summary()\n",
        "\n",
        "                                history = model.fit(\n",
        "                                    train_aug,\n",
        "                                    validation_data=validation_aug,\n",
        "                                    epochs=EPOCHS,\n",
        "                                    callbacks=[CHECKPOINT, EARLYSTOP],\n",
        "                                    verbose=VERBOSE\n",
        "                                )\n",
        "\n",
        "                                print(\n",
        "                                    '[INFO] Model:\\n\\tBase model: ', BASE_MODEL['NAME'], \n",
        "                                    '\\n\\tEpochs: ', EPOCHS, \n",
        "                                    '\\n\\tLearning rate: ', LR, \n",
        "                                    '\\n\\tBatch size: ', BATCH_SIZE, \n",
        "                                    '\\n\\tNum units: ', NUM_UNITS, \n",
        "                                    '\\n\\tActivation: ', ACTIVATION, \n",
        "                                    '\\n\\tDropout rate: ', DROPOUT_RATE, \n",
        "                                    '\\n\\tRegularizer: ', REGULARIZER['NAME']\n",
        "                                )\n",
        "                                print(\n",
        "                                    '[WEIGHTS] Model:\\n\\tSum of the weights of first dense layer: ', sum(sum(abs(model.layers[3].get_weights()[0]))), \n",
        "                                    '\\n\\tSum of the biases of first dense layer: ', sum(abs(model.layers[3].get_weights()[1])), \n",
        "                                    '\\n\\tSum of the weights of second dense layer: ', sum(sum(abs(model.layers[5].get_weights()[0]))), \n",
        "                                    '\\n\\tSum of the biases of second dense layer: ', sum(abs(model.layers[5].get_weights()[1]))\n",
        "                                )\n",
        "                                print(\n",
        "                                    '[TRAINING] Model:\\n\\tTrain loss: ', history.history['loss'][-1], \n",
        "                                    '\\n\\tTrain accuracy: ', history.history['accuracy'][-1], \n",
        "                                    '\\n\\tTrain precision: ', history.history['precision'][-1], \n",
        "                                    '\\n\\tTrain recall: ', history.history['recall'][-1], \n",
        "                                    '\\n\\tTrain f1-score: ', history.history['f1_score'][-1]\n",
        "                                )\n",
        "                                print(\n",
        "                                    '[VALIDATING] Model:\\n\\tValidation loss: ', history.history['val_loss'][-1], \n",
        "                                    '\\n\\tValidation accuracy: ', history.history['val_accuracy'][-1], \n",
        "                                    '\\n\\tValidation precision: ', history.history['val_precision'][-1], \n",
        "                                    '\\n\\tValidation recall: ', history.history['val_recall'][-1], \n",
        "                                    '\\n\\tValidation f1-score: ', history.history['val_f1_score'][-1]\n",
        "                                )\n",
        "                                plot_history(history)\n",
        "\n",
        "                                results_writer.writerow([\n",
        "                                    EPOCHS, \n",
        "                                    BATCH_SIZE, \n",
        "                                    BASE_MODEL['NAME'], \n",
        "                                    DROPOUT_RATE, \n",
        "                                    LR, \n",
        "                                    NUM_UNITS, \n",
        "                                    ACTIVATION, \n",
        "                                    REGULARIZER['NAME'], \n",
        "                                    history.history['loss'][-1], \n",
        "                                    history.history['accuracy'][-1], \n",
        "                                    history.history['precision'][-1], \n",
        "                                    history.history['recall'][-1], \n",
        "                                    history.history['f1_score'][-1], \n",
        "                                    history.history['val_loss'][-1], \n",
        "                                    history.history['val_accuracy'][-1], \n",
        "                                    history.history['val_precision'][-1],\n",
        "                                    history.history['val_recall'][-1], \n",
        "                                    history.history['val_f1_score'][-1]]\n",
        "                                )\n",
        "                                model.save(DATA_PATH + 'model_' + str(EPOCHS) + '_' + str(BATCH_SIZE) + '_' + BASE_MODEL['NAME'] + '_' + str(DROPOUT_RATE) + '_' + str(LR) + '_' + str(NUM_UNITS) + '_' + ACTIVATION + '_' + REGULARIZER['NAME'] + '.hdf5')                        \n",
        "                                \n",
        "                                tensorflow.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ViSOHf7UoE"
      },
      "source": [
        "### Structured approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzuNFlcs7YUY"
      },
      "source": [
        "EPOCHS = 10\n",
        "MAX_TRIALS = 64\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=MAX_TRIALS,\n",
        "    executions_per_trial=1,\n",
        "    directory=DATA_PATH,\n",
        "    project_name='tuner'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    train_aug,\n",
        "    validation_data=validation_aug,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[\n",
        "        EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=PATIENCE,\n",
        "        restore_best_weights=True)\n",
        "    ],\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "tuner.results_summary(num_trials=MAX_TRIALS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEnxBziK7p3x"
      },
      "source": [
        "best_model = tuner.get_best_models()[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
        "best_model.save(DATA_PATH + 'best_model.hdf5')\n",
        "best_model = load_model(\n",
        "    DATA_PATH + 'best_model.hdf5',\n",
        "    custom_objects={\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        ")\n",
        "best_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypzOdhO2jH4D"
      },
      "source": [
        "### Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUy1pzrxPqNF"
      },
      "source": [
        "EPOCHS = 30\n",
        "BASE_MODEL = clone_model(model_resnet)\n",
        "BASE_MODEL.set_weights(model_resnet.get_weights())\n",
        "INITIAL_SPARSITY = 0.2\n",
        "FINAL_SPARSITY = 0.8\n",
        "NUM_UNITS = 896\n",
        "DROPOUT_RATE = 0.6\n",
        "ACTIVATION = 'elu'\n",
        "L2_FACTOR = 3e-1\n",
        "LR = 2e-5\n",
        "BETA_1 = 0.95\n",
        "BETA_2 = 0.9987\n",
        "\n",
        "pruning_params = {\n",
        "    'pruning_schedule': sparsity.PolynomialDecay(\n",
        "        initial_sparsity=INITIAL_SPARSITY,\n",
        "        final_sparsity=FINAL_SPARSITY,\n",
        "        begin_step=0,\n",
        "        end_step=(np.ceil(1.0*train.shape[0]/BATCH_SIZE).astype(np.int32)*EPOCHS),\n",
        "        frequency=100\n",
        "    )\n",
        "}\n",
        "\n",
        "pruned_model = Sequential([\n",
        "    BASE_MODEL,\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    sparsity.prune_low_magnitude(Dense(\n",
        "        NUM_UNITS, \n",
        "        activation=ACTIVATION, \n",
        "        kernel_regularizer=l2(L2_FACTOR)\n",
        "    ), **pruning_params),\n",
        "    Dropout(DROPOUT_RATE),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "pruned_model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(\n",
        "        lr=LR, \n",
        "        beta_1=BETA_1, \n",
        "        beta_2=BETA_2\n",
        "    ),\n",
        "    metrics=[precision, recall, f1_score, AUC(), 'accuracy']\n",
        ")\n",
        "\n",
        "history = pruned_model.fit(\n",
        "    train_aug,\n",
        "    validation_data=validation_aug,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=VERBOSE,\n",
        "    callbacks=[\n",
        "        sparsity.UpdatePruningStep(),\n",
        "        sparsity.PruningSummaries(log_dir=(DATA_PATH+'pruning/')),\n",
        "    ]\n",
        ")\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blh7V5uGNaWr"
      },
      "source": [
        "pruned_model = sparsity.strip_pruning(pruned_model)\n",
        "\n",
        "pruned_model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(\n",
        "        lr=LR, \n",
        "        beta_1=BETA_1, \n",
        "        beta_2=BETA_2\n",
        "    ),\n",
        "    metrics=[precision, recall, f1_score, AUC(), 'accuracy']\n",
        ")\n",
        "\n",
        "pruned_model.save(DATA_PATH + 'pruned_model.hdf5')\n",
        "pruned_model = load_model(\n",
        "    DATA_PATH + 'pruned_model.hdf5',\n",
        "    custom_objects={\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        ")\n",
        "pruned_model.summary()\n",
        "\n",
        "for i, w in enumerate(pruned_model.get_weights()):\n",
        "    if i == 270:\n",
        "        print(\"{} - Total params: {}, Zeros: {:.2f}%\".format(pruned_model.weights[i].name, w.size, np.sum(w==0)/w.size*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmuZ4Y58DfYC"
      },
      "source": [
        "## Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLXiZl1DDfYC"
      },
      "source": [
        "score_best_model = best_model.evaluate(test_aug, verbose=VERBOSE)\n",
        "score_pruned_model = pruned_model.evaluate(test_aug, verbose=VERBOSE)\n",
        "\n",
        "print(\n",
        "    '\\n[TESTING] Best Model:\\n\\tTest loss: ', score_best_model[0], \n",
        "    '\\n\\tTest accuracy: ', score_best_model[5], \n",
        "    '\\n\\tTest precision: ', score_best_model[1], \n",
        "    '\\n\\tTest recall: ', score_best_model[2], \n",
        "    '\\n\\tTest f1-score: ', score_best_model[3], \n",
        "    '\\n\\tTest auc: ', score_best_model[4],\n",
        "    '\\n\\n[TESTING] Pruned Model:\\n\\tTest loss: ', score_pruned_model[0], \n",
        "    '\\n\\tTest accuracy: ', score_pruned_model[5], \n",
        "    '\\n\\tTest precision: ', score_pruned_model[1], \n",
        "    '\\n\\tTest recall: ', score_pruned_model[2], \n",
        "    '\\n\\tTest f1-score: ', score_pruned_model[3], \n",
        "    '\\n\\tTest auc: ', score_pruned_model[4]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}