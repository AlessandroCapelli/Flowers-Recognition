{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Source Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0OfFtAWDfXm"
      },
      "source": [
        "# Project of Advanced Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxOgXMOVDfXt"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjjrv2wenury"
      },
      "source": [
        "pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcwZHDfjDfXv"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "import tarfile\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "tensorflow.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE5WblbCDfXw"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l9cxCJuDfXx"
      },
      "source": [
        "def get_image_names(tgz):\n",
        "    with tarfile.open(tgz) as file:\n",
        "        return [i.name for i in file.getmembers() if i.isfile()]\n",
        "\n",
        "def show_random_images(n=4, size=10, label=None):\n",
        "    if(label != None):\n",
        "        images = df[df['label'] == str(label)].sample(n=(n*n))\n",
        "    else:\n",
        "        images = df.sample(n=(n*n))\n",
        "        \n",
        "    plt.figure(figsize=(size, size))\n",
        "    for index, path in enumerate(images['id'].values):\n",
        "        plt.subplot(n, n, index+1)\n",
        "        plt.imshow(PIL.Image.open(DATA_PATH + 'images/' + path))\n",
        "        plt.title('Label: ' + str(images['label'].values[index]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def show_random_images_aug(aug, n=2, size=10):\n",
        "    x, y = aug.next()\n",
        "    \n",
        "    plt.figure(figsize=(size, size))\n",
        "    for i in range(0, (n*n)):\n",
        "        plt.subplot(n, n, i+1)\n",
        "        plt.imshow(x[i])\n",
        "        plt.title('Label: ' + str(np.where(y[i] == 1)[0][0]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['accuracy'], 'orange', label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], 'royalblue', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['precision'], 'orange', label='Training Precision')\n",
        "    plt.plot(history.history['val_precision'], 'royalblue', label='Validation Precision')\n",
        "    plt.title('Training and Validation Precision')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['recall'], 'orange', label='Training Recall')\n",
        "    plt.plot(history.history['val_recall'], 'royalblue', label='Validation Recall')\n",
        "    plt.title('Training and Validation Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['f1_score'], 'orange', label='Training F1-Score')\n",
        "    plt.plot(history.history['val_f1_score'], 'royalblue', label='Validation F1-Score')\n",
        "    plt.title('Training and Validation F1-Score')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1-Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['loss'], 'orange', label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], 'royalblue', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def true_positives(y_true, y_pred):\n",
        "    return tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    predicted_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_pred, 0, 1)))\n",
        "\n",
        "    return true_positives(y_true, y_pred) / (predicted_positives + tensorflow.keras.backend.epsilon())\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    possible_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_true, 0, 1)))\n",
        "\n",
        "    return true_positives(y_true, y_pred) / (possible_positives + tensorflow.keras.backend.epsilon())\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    possible_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_true, 0, 1)))\n",
        "    predicted_positives = tensorflow.keras.backend.sum(tensorflow.keras.backend.round(tensorflow.keras.backend.clip(y_pred, 0, 1)))\n",
        "\n",
        "    return 2*(precision(y_true, y_pred)*recall(y_true, y_pred))/(precision(y_true, y_pred)+recall(y_true, y_pred)+tensorflow.keras.backend.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgdgpW-oDfXy"
      },
      "source": [
        "## Configuration parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WptElZuQEbWs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXhIxp-tDfX0"
      },
      "source": [
        "DATA_PATH = '/content/drive/My Drive/data/' #  './data/'\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "TEST_RATIO = 0.3\n",
        "VALIDATION_RATIO = 0.2\n",
        "NUM_CLASSES = 102\n",
        "IMG_SIZE = 250\n",
        "IMG_CHANNELS = 3\n",
        "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "VERBOSE = 1\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 5\n",
        "\n",
        "CHECKPOINT = ModelCheckpoint((DATA_PATH + 'model.hdf5'), monitor=['val_accuracy'], verbose=VERBOSE, mode='max')\n",
        "EARLYSTOP = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D_pQ291DfX0"
      },
      "source": [
        "## Base models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XPBeUxQDfX0"
      },
      "source": [
        "model_vgg = VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
        "model_vgg.trainable = False\n",
        "\n",
        "model_vgg.summary()\n",
        "\n",
        "model_xception = Xception(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
        "model_xception.trainable = False\n",
        "\n",
        "model_xception.summary()\n",
        "\n",
        "model_resnet = ResNet50V2(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
        "model_resnet.trainable = False\n",
        "\n",
        "model_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of9vKuJYDfX3"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3zJszVJDfX3"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['id'] = sorted(get_image_names(DATA_PATH + 'images.tgz'))\n",
        "df['label'] = scipy.io.loadmat(DATA_PATH + 'labels.mat')['labels'][0] - 1\n",
        "df['label'] = df['label'].astype('str')\n",
        "\n",
        "tarfile.open(DATA_PATH + 'images.tgz').extractall(DATA_PATH + 'images/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M9eZEidDfX4"
      },
      "source": [
        "print('Size of dataset:')\n",
        "print(df.count())\n",
        "\n",
        "print('\\nList of images:')\n",
        "print(os.listdir(DATA_PATH + 'images/jpg')[:10])\n",
        "\n",
        "print('\\nDataframe:')\n",
        "print(df.head(10))\n",
        "\n",
        "print('\\nNumber of classes:')\n",
        "print(df['label'].nunique())\n",
        "\n",
        "print('\\nImages for each class:')\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "print('\\nSample images:')\n",
        "show_random_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8_KHGSbDfX6"
      },
      "source": [
        "## Dataset split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7n9X16vDfX6"
      },
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(\n",
        "    df['id'], \n",
        "    df['label'], \n",
        "    test_size=TEST_RATIO, \n",
        "    random_state=SEED, \n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "train_X, validation_X, train_y, validation_y = train_test_split(\n",
        "    train_X, \n",
        "    train_y, \n",
        "    test_size=(VALIDATION_RATIO/(1-TEST_RATIO)), \n",
        "    random_state=SEED,\n",
        "    stratify=train_y\n",
        ")\n",
        "\n",
        "train = pd.DataFrame(train_X)\n",
        "train['label'] = train_y\n",
        "\n",
        "validation = pd.DataFrame(validation_X)\n",
        "validation['label'] = validation_y\n",
        "\n",
        "test = pd.DataFrame(test_X)\n",
        "test['label'] = test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOaUF4oVDfX7"
      },
      "source": [
        "print('Size of train set:')\n",
        "print(train.shape)\n",
        "\n",
        "print('\\nDataframe of the train set:')\n",
        "print(train.head(10))\n",
        "\n",
        "print('\\nImages for each class of the train set:')\n",
        "print(train['label'].value_counts())\n",
        "\n",
        "print('\\nSize of validation set:')\n",
        "print(validation.shape)\n",
        "\n",
        "print('\\nDataframe of the validation set:')\n",
        "print(validation.head(10))\n",
        "\n",
        "print('\\nImages for each class of the validation set:')\n",
        "print(validation['label'].value_counts())\n",
        "\n",
        "print('\\nSize of test set:')\n",
        "print(test.shape)\n",
        "\n",
        "print('\\nDataframe of the test set:')\n",
        "print(test.head(10))\n",
        "\n",
        "print('\\nImages for each class of the test set:')\n",
        "print(test['label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7jzcvZ3DfX7"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LULYp5bNDfX7",
        "scrolled": false
      },
      "source": [
        "pd.DataFrame(df['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eh__ouoDfX8"
      },
      "source": [
        "pd.DataFrame(train['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYuHxLzaDfX9"
      },
      "source": [
        "pd.DataFrame(validation['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eule5C0yDfX9"
      },
      "source": [
        "pd.DataFrame(test['label'].value_counts(sort=True)).plot(kind='barh', figsize=(10, 20), color='royalblue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9niX7G_DfX-"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9g5XrBQDfX-"
      },
      "source": [
        "train_aug_datagen = ImageDataGenerator(\n",
        "    rotation_range=45, \n",
        "    width_shift_range=0.1, \n",
        "    height_shift_range=0.1, \n",
        "    brightness_range=[0.5, 1.5], \n",
        "    shear_range=0.15, \n",
        "    zoom_range=[0.75, 1.25], \n",
        "    fill_mode=\"nearest\", \n",
        "    horizontal_flip=True, \n",
        "    rescale=1./255\n",
        ")\n",
        "train_aug = train_aug_datagen.flow_from_dataframe(\n",
        "    dataframe=train, \n",
        "    directory=(DATA_PATH + 'images/'), \n",
        "    x_col='id', \n",
        "    y_col='label', \n",
        "    target_size=(IMG_SIZE, IMG_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "validation_aug_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "validation_aug = validation_aug_datagen.flow_from_dataframe(\n",
        "    dataframe=validation, \n",
        "    directory=(DATA_PATH + 'images/'), \n",
        "    x_col='id', \n",
        "    y_col='label', \n",
        "    target_size=(IMG_SIZE, IMG_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "test_aug_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "test_aug = test_aug_datagen.flow_from_dataframe(\n",
        "    dataframe=test, \n",
        "    directory=(DATA_PATH + 'images/'), \n",
        "    x_col='id', \n",
        "    y_col='label', \n",
        "    target_size=(IMG_SIZE, IMG_SIZE), \n",
        "    batch_size=BATCH_SIZE, \n",
        "    class_mode='categorical', \n",
        "    shuffle=True, \n",
        "    seed=SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJypoYbADfX-",
        "scrolled": false
      },
      "source": [
        "print('Sample augmented images of the train set:')\n",
        "show_random_images_aug(aug=train_aug)\n",
        "\n",
        "print('Sample augmented images of the validation set:')\n",
        "show_random_images_aug(aug=validation_aug)\n",
        "\n",
        "print('Sample augmented images of the test set:')\n",
        "show_random_images_aug(aug=test_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjcuvaqaDfX_"
      },
      "source": [
        "## Model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogdjWGUEDfX_"
      },
      "source": [
        "tensorflow.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D5NCVS-8J45"
      },
      "source": [
        "### Model creation for manual tuning of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkDW8jfdDfYA",
        "scrolled": true
      },
      "source": [
        "def create_model(BASE_MODEL, LR, DROPOUT_RATE, NUM_UNITS, ACTIVATION, REGULARIZER=None):\n",
        "    BASE_MODEL_TEMP = BASE_MODEL\n",
        "    BASE_MODEL_TEMP.trainable = True\n",
        "\n",
        "    # fine_tune_at = int(round(len(BASE_MODEL_TEMP.layers)))\n",
        "    # for layer in BASE_MODEL_TEMP.layers[:fine_tune_at]:\n",
        "    #     layer.trainable = False\n",
        "    \n",
        "    # for layer in BASE_MODEL_TEMP.layers: \n",
        "    #     layer.build(layer.input_shape)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(BASE_MODEL_TEMP)\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(NUM_UNITS, activation=ACTIVATION, kernel_regularizer=REGULARIZER))\n",
        "    model.add(Dropout(DROPOUT_RATE))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', \n",
        "        optimizer=Adam(lr=LR), \n",
        "        metrics=[precision, recall, f1_score, AUC(), 'accuracy']\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_LYg48w8e5x"
      },
      "source": [
        "### Model creation for automatic tuning of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkjVT1t38iBS"
      },
      "source": [
        "def build_model(hp):\n",
        "    BASE_MODEL_TEMP = model_resnet\n",
        "    BASE_MODEL_TEMP.trainable = True\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(BASE_MODEL_TEMP)\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(hp.Int('NUM_UNITS', 128, 1024, step=128, default=256), activation=hp.Choice(f'ACTIVATION', ['elu', 'relu']), kernel_regularizer=l2(3e-1)))\n",
        "    model.add(Dropout(hp.Float(f'DROPOUT_RATE', 0.1, 0.7, step=0.1, default=0.5)))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', \n",
        "        optimizer=Adam(lr=hp.Choice(f\"LR\", [1e-5, 2e-5, 5e-5, 5e-6])), \n",
        "        metrics=[precision, recall, f1_score, AUC(), 'accuracy']\n",
        "    )\n",
        "  \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNp7EooBDfYA"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPYcaC0x7JSH"
      },
      "source": [
        "### Naive approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejsEJO1wDfYA"
      },
      "source": [
        "PARAMS = {\n",
        "    'BASE_MODEL': [{'MODEL': model_resnet, 'NAME': 'RESNET50V2'}], # [{'MODEL': model_vgg, 'NAME': 'VGG16'}, {'MODEL':  model_xception, 'NAME': 'XCEPTION'}, {'MODEL': model_resnet, 'NAME': 'RESNET50V2'}],\n",
        "    'LR': [2e-5], # [1e-3, 1e-4],\n",
        "    'EPOCHS': [50],\n",
        "    'DROPOUT_RATE': [0.45], # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
        "    'NUM_UNITS': [256], # [128, 256, 512],\n",
        "    'ACTIVATION': ['elu'], # ['elu', 'relu'],\n",
        "    'REGULARIZER': [{'REGULARIZER': l2(3e-1), 'NAME': 'L2(0.3)'}] #Â [{'REGULARIZER': l1(5e-3), 'NAME': 'L1(0.005)'}, {'REGULARIZER': l2(1e-2), 'NAME': 'L2(0.01)'}, {'REGULARIZER': l1(5e-4), 'NAME': 'L1(0.0005)'}, {'REGULARIZER': l2(1e-3), 'NAME': 'L2(0.001)'}]\n",
        "}\n",
        "\n",
        "with open(DATA_PATH + 'results.csv', mode='w') as results:\n",
        "    results_writer = csv.writer(results, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    results_writer.writerow(['EPOCHS', 'BATCH SIZE', 'BASE MODEL', 'DROPOUT RATE', 'LR', 'NUM UNITS', 'ACTIVATION', 'REGULARIZER', 'LOSS', 'ACCURACY', 'PRECISION', 'RECALL', 'F1-SCORE', 'VALIDATION LOSS', 'VALIDATION ACCURACY', 'VALIDATION PRECISION', 'VALIDATION RECALL', 'VALIDATION F1-SCORE'])\n",
        "\n",
        "for EPOCHS in PARAMS['EPOCHS']:\n",
        "    for BASE_MODEL in PARAMS['BASE_MODEL']:\n",
        "        for DROPOUT_RATE in PARAMS['DROPOUT_RATE']:\n",
        "            for REGULARIZER in PARAMS['REGULARIZER']:\n",
        "                for LR in PARAMS['LR']:\n",
        "                    for NUM_UNITS in PARAMS['NUM_UNITS']:\n",
        "                        for ACTIVATION in PARAMS['ACTIVATION']:\n",
        "                            with open(DATA_PATH + 'results.csv', mode='a') as results:\n",
        "                                results_writer = csv.writer(results, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "                                model = create_model(BASE_MODEL['MODEL'], LR, DROPOUT_RATE, NUM_UNITS, ACTIVATION, REGULARIZER['REGULARIZER'])\n",
        "                                model.summary()\n",
        "\n",
        "                                history = model.fit(\n",
        "                                    train_aug,\n",
        "                                    validation_data=validation_aug,\n",
        "                                    epochs=EPOCHS,\n",
        "                                    callbacks=[CHECKPOINT, EARLYSTOP],\n",
        "                                    verbose=VERBOSE\n",
        "                                )\n",
        "\n",
        "                                print('[INFO] Model:\\n\\tBase model: ', BASE_MODEL['NAME'], '\\n\\tEpochs: ', EPOCHS, '\\n\\tLearning rate: ', LR, '\\n\\tBatch size: ', BATCH_SIZE, '\\n\\tNum units: ', NUM_UNITS, '\\n\\tActivation: ', ACTIVATION, '\\n\\tDropout rate: ', DROPOUT_RATE, '\\n\\tRegularizer: ', REGULARIZER['NAME'])\n",
        "                                print('[WEIGHTS] Model:\\n\\tSum of the weights of first dense layer: ', sum(sum(abs(model.layers[3].get_weights()[0]))), '\\n\\tSum of the biases of first dense layer: ', sum(abs(model.layers[3].get_weights()[1])), '\\n\\tSum of the weights of second dense layer: ', sum(sum(abs(model.layers[5].get_weights()[0]))), '\\n\\tSum of the biases of second dense layer: ', sum(abs(model.layers[5].get_weights()[1])))\n",
        "                                print('[TRAINING] Model:\\n\\tTrain loss: ', history.history['loss'][-1], '\\n\\tTrain accuracy: ', history.history['accuracy'][-1], '\\n\\tTrain precision: ', history.history['precision'][-1], '\\n\\tTrain recall: ', history.history['recall'][-1], '\\n\\tTrain f1-score: ', history.history['f1_score'][-1])\n",
        "                                print('[VALIDATING] Model:\\n\\tValidation loss: ', history.history['val_loss'][-1], '\\n\\tValidation accuracy: ', history.history['val_accuracy'][-1], '\\n\\tValidation precision: ', history.history['val_precision'][-1], '\\n\\tValidation recall: ', history.history['val_recall'][-1], '\\n\\tValidation f1-score: ', history.history['val_f1_score'][-1])\n",
        "                                plot_history(history)\n",
        "\n",
        "                                results_writer.writerow([EPOCHS, BATCH_SIZE, BASE_MODEL['NAME'], DROPOUT_RATE, LR, NUM_UNITS, ACTIVATION, REGULARIZER['NAME'], history.history['loss'][-1], history.history['accuracy'][-1], history.history['precision'][-1], history.history['recall'][-1], history.history['f1_score'][-1], history.history['val_loss'][-1], history.history['val_accuracy'][-1], history.history['val_precision'][-1], history.history['val_recall'][-1], history.history['val_f1_score'][-1]])\n",
        "                                model.save(DATA_PATH + 'model_' + str(EPOCHS) + '_' + str(BATCH_SIZE) + '_' + BASE_MODEL['NAME'] + '_' + str(DROPOUT_RATE) + '_' + str(LR) + '_' + str(NUM_UNITS) + '_' + ACTIVATION + '_' + REGULARIZER['NAME'] + '.hdf5')                        \n",
        "                                \n",
        "                                tensorflow.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ViSOHf7UoE"
      },
      "source": [
        "### Structured approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzuNFlcs7YUY"
      },
      "source": [
        "EPOCHS = 10\n",
        "MAX_TRIALS = 32\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=MAX_TRIALS,\n",
        "    directory=DATA_PATH,\n",
        "    project_name='FlowersRecognition'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    train_aug, \n",
        "    validation_data=validation_aug, \n",
        "    epochs=EPOCHS, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=PATIENCE, restore_best_weights=True)],\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEnxBziK7p3x"
      },
      "source": [
        "model = tuner.get_best_models()[0]\n",
        "model.save(DATA_PATH + 'model.hdf5')\n",
        "model = tensorflow.keras.models.load_model(DATA_PATH + 'model.hdf5', custom_objects={'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmuZ4Y58DfYC"
      },
      "source": [
        "## Model testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLXiZl1DDfYC"
      },
      "source": [
        "score = model.evaluate(test_aug, verbose=VERBOSE)\n",
        "print('[TESTING] Model:\\n\\tTest loss: ', score[0], '\\n\\tTest accuracy: ', score[5], '\\n\\tTest precision: ', score[1], '\\n\\tTest recall: ', score[2], '\\n\\tTest f1-score: ', score[3], '\\n\\tTest auc: ', score[4])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}